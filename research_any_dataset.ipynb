{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Single Fingerprint Deep Classification**\n",
    "### **JCE - Software Engineering Final Project** \n",
    "##### ***By Kobi Amsellem & Zohar Kedem***\n",
    "\n",
    "##### In this study we want to discovare if Deep Convolutional Neural Network can classify single fingerprint image to find the owner: \n",
    "##### **1. Gender** - 2 classes (male/female).\n",
    "##### **2. Finger name** - 10 classes (right-thumb, ..., right-thumb, ...)\n",
    "##### **3. Fingerprint type** - 5 classes (left loop, whirl, right loop, tented arch, arch)\n",
    "##### **4. Same Person** - 2 classes (Same, Different) *(whether or not two fingerprints belong to the same personperson)* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import datasets as ds\n",
    "import tensorflow as tf\n",
    "import FPMLmodule.utils as utils\n",
    "import FPMLmodule.backbones as bb\n",
    "import FPMLmodule.classifiers as cf\n",
    "from tensorflow.keras.optimizers import Adam, Nadam, RMSprop\n",
    "from FPMLmodule.fpml import FPML \n",
    "from pathlib import Path\n",
    "\n",
    "print('TensorFlow Version:', tf.__version__)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "tf.random.set_seed(9)\n",
    "dbStudyOutPath ='./out/{}/'\n",
    "\n",
    "weightsRN50 = \"./weights/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
    "weightsMNV2 = \"./weights/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\"\n",
    "weightsENB2 = \"./weights/efficientnetb2_notop.h5\"\n",
    "weightsINCEPTIONV3 = \"./weights/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
    "weightsXCEPTION = \"./weights/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Model Search** \n",
    "##### Main method to search after the best model\n",
    "\n",
    "in this section we define the ````researchBestModel````, a method to search and compare between several different models and hyper parameters and choose the best one for specific dataset.\n",
    "\n",
    "##### **Stages:**\n",
    "##### **1.** Backbone Transfer Learning mode comparison - (a) Pretraind wheights and Untrainable, (b) Pretraind wheights and Trainable, (c) Initialize wheight and Trainable.\n",
    "##### **2.** Backbone comparison. \n",
    "##### **3.** Classifier (perceptron) comparison. \n",
    "##### **4.** Loss function comparison. \n",
    "##### **5.** Learning rates comparison. \n",
    "##### **6.** Optimizers comparison. \n",
    "##### **7.** Train best configuration for extra epochs on train+validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def researchBestModel(configureDS, datasets, epochsSearch, epochsBest, path, optimizers, \n",
    "                      learningRates, defaultHypers, losses, classifiers, defaultClassifier, \n",
    "                      defaultBackbone, backbonesForSearch, verbose=1, top=1):\n",
    "    trainDs, testDs, valDs = datasets\n",
    "    imgDim = configureDS.inputDim\n",
    "    nbClasses = len(configureDS.classNames)\n",
    "    outFilePath = path + 'ablationHistory.csv'\n",
    "    topModelsPath = path+'FinalModels/'\n",
    "    \n",
    "    print('Ablation for', configureDS.name)\n",
    "    \n",
    "    \n",
    "    baseStudy = {\n",
    "            \"architecture\": {\n",
    "                \"backbone\": defaultBackbone['backbone'](imgDim, weights=defaultBackbone['weights'], trainable=False),\n",
    "                \"classifier\": defaultClassifier(nbClasses, \"softmax\"),\n",
    "                \"inputLayer\": None, \n",
    "                \"inputDim\": imgDim\n",
    "            },\n",
    "            \"hyperparameters\": defaultHypers\n",
    "        }\n",
    "    \n",
    "    \n",
    "    \n",
    "    histories = {}\n",
    "    \n",
    "    # --------------------------------------------------\n",
    "    # ---- Stage 1 - Compare Transfer Learning Mode ----\n",
    "    # --------------------------------------------------\n",
    "    \n",
    "    trainingModeStudy = baseStudy\n",
    "    trainingModeStudy['architecture']['backbone'] = [\n",
    "                    defaultBackbone['backbone'](imgDim, weights=defaultBackbone['weights'], trainable=False, name='PT&Untrainable'),\n",
    "                    defaultBackbone['backbone'](imgDim, weights=defaultBackbone['weights'], trainable=True, name='PT&Trainable'),\n",
    "                    defaultBackbone['backbone'](imgDim, weights=None, trainable=True, name='IN&Trainable')\n",
    "                    ]    \n",
    "    trainingModeHistories = utils.researchStudies(trainDs, valDs, {'DNN-TransferLearningMode' : trainingModeStudy}, epochsSearch, verbose)\n",
    "    for modelName, model in trainingModeHistories.items():\n",
    "        model['model'].save(path+\"search_\"+modelName)\n",
    "    histories = {**histories, **trainingModeHistories}\n",
    "    bestTrainMode = utils.getBestStudyFromHistories(trainingModeHistories)\n",
    "    trainable, pretrained = 'Trainable' in bestTrainMode, 'PT' in bestTrainMode\n",
    "    utils.displayStudiesProgress(trainingModeHistories, path, 'Comparison Transfer Learning Mode')\n",
    "    utils.saveStudyHistory({'DNN-TransferLearningMode' : trainingModeStudy}, trainingModeHistories, outFilePath)\n",
    "    print(\"Backbone Transfer Learning Mode Selected - Pretrained:\", pretrained, 'Trainable:', trainable)\n",
    "    \n",
    "\n",
    "    \n",
    "    # ------------------------------------------\n",
    "    # ---- Stage 2 - Compare DNN Backbones -----\n",
    "    # ------------------------------------------\n",
    "    backboneConfigs = [backbone['backbone'](imgDim, weights=backbone['weights'], trainable=trainable) for backbone in backbonesForSearch]\n",
    "    bestBackboneStudy = copy.deepcopy(trainingModeHistories[bestTrainMode]['config'])\n",
    "    bestBackboneStudy[\"architecture\"][\"backbone\"] = backboneConfigs\n",
    "    bestBackboneHistories = utils.researchStudies(trainDs, valDs, {'Backbone': bestBackboneStudy}, epochsSearch, verbose)\n",
    "    for modelName, model in bestBackboneHistories.items():\n",
    "        model['model'].save(path+\"search_\"+modelName)\n",
    "    histories = {**histories, **bestBackboneHistories}\n",
    "    bestBackboneHistories['Backbone'+\"_\"+defaultBackbone['backbone'].__name__] = trainingModeHistories[bestTrainMode]\n",
    "    bestBackbone = utils.getBestStudyFromHistories(bestBackboneHistories)\n",
    "    utils.displayStudiesProgress(bestBackboneHistories, path, 'Comparison DNN Backbone')\n",
    "    utils.saveStudyHistory({'BestBackbone': bestBackboneStudy}, bestBackboneHistories, outFilePath)\n",
    "    print(\"Best DNN Backbone:\", bestBackboneHistories[bestBackbone]['config'][\"architecture\"][\"backbone\"].name)\n",
    "\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # ---- Stage 3 - Compare FC Classifiers ----\n",
    "    # --------------------------------------------\n",
    "    classifierStudy = copy.deepcopy(bestBackboneHistories[bestBackbone]['config'])\n",
    "    classifierStudy['architecture'][\"classifier\"] = [classifier(nbClasses, \"softmax\") for classifier in classifiers]\n",
    "    classifierHistories = utils.researchStudies(trainDs, valDs, {'Classifiers': classifierStudy}, epochsSearch, verbose)\n",
    "    for modelName, model in classifierHistories.items():\n",
    "        model['model'].save(path+\"search_\"+modelName)\n",
    "    histories = {**histories, **classifierHistories}\n",
    "    classifierHistories['Classifier'+\"_\"+str(defaultClassifier.__name__)] = bestBackboneHistories[bestBackbone]\n",
    "    bestClassifier = utils.getBestStudyFromHistories(classifierHistories)\n",
    "    utils.displayStudiesProgress(classifierHistories, path, 'Comparison Classifiers')\n",
    "    utils.saveStudyHistory({'bestClassifier': classifierStudy}, classifierHistories, outFilePath)\n",
    "    print(\"Best Classifier:\", str(classifierHistories[bestClassifier]['config']['architecture'][\"classifier\"]))\n",
    "    \n",
    "    # ------------------------------------------\n",
    "    # ---- Stage 4 - Compare Loss Functions ----\n",
    "    # ------------------------------------------\n",
    "    lossStudy = copy.deepcopy(classifierHistories[bestClassifier]['config'])\n",
    "    lossStudy['hyperparameters'][\"loss\"] = losses\n",
    "    lossesHistories = utils.researchStudies(trainDs, valDs, {'LossFunction': lossStudy}, epochsSearch, verbose)\n",
    "    for modelName, model in lossesHistories.items():\n",
    "        model['model'].save(path+\"search_\"+modelName)\n",
    "    histories = {**histories, **lossesHistories}\n",
    "    lossesHistories['LossFunction'+\"_\"+str(defaultHypers[\"loss\"])] = classifierHistories[bestClassifier]\n",
    "    bestLoss = utils.getBestStudyFromHistories(lossesHistories)\n",
    "    utils.displayStudiesProgress(lossesHistories, path, 'Comparison Loss Functions')\n",
    "    utils.saveStudyHistory({'BestLoss': lossStudy}, lossesHistories, outFilePath)\n",
    "    print(\"Best Loss Function:\", str(lossesHistories[bestLoss]['config']['hyperparameters'][\"loss\"]))\n",
    "    \n",
    "    \n",
    "    # ------------------------------------------\n",
    "    # ---- Stage 5 - Compare Learning Rates ----\n",
    "    # ------------------------------------------\n",
    "    learningRatesStudy = copy.deepcopy(lossesHistories[bestLoss]['config'])\n",
    "    learningRatesStudy['hyperparameters']['learningRate'] = learningRates\n",
    "    learningRatesHistories = utils.researchStudies(trainDs, valDs, {'LearningRate': learningRatesStudy}, epochsSearch, verbose)\n",
    "    for modelName, model in learningRatesHistories.items():\n",
    "        model['model'].save(path+\"search_\"+modelName)\n",
    "    histories = {**histories, **learningRatesHistories}\n",
    "    learningRatesHistories['LearningRate'+\"_\"+str(defaultHypers[\"learningRate\"])] = lossesHistories[bestLoss]\n",
    "    bestLearningRate = utils.getBestStudyFromHistories(learningRatesHistories)\n",
    "    utils.displayStudiesProgress(learningRatesHistories, path, 'Comparison Learning Rate')\n",
    "    utils.saveStudyHistory({'BestLearningRate': learningRatesStudy}, learningRatesHistories, outFilePath)\n",
    "    print(\"Best Learning Rate:\", str(learningRatesHistories[bestLearningRate]['config'][\"hyperparameters\"][\"learningRate\"]))\n",
    "\n",
    "\n",
    "    # --------------------------------------\n",
    "    # ---- Stage 6 - Compare Optimizers ----\n",
    "    # --------------------------------------\n",
    "    optimizersStudy = copy.deepcopy(learningRatesHistories[bestLearningRate]['config'])\n",
    "    optimizersStudy[\"hyperparameters\"][\"optimizer\"] = optimizers\n",
    "    optimizersHistories = utils.researchStudies(trainDs, valDs, {'Optimizer' : optimizersStudy}, epochsSearch, verbose)\n",
    "    for modelName, model in optimizersHistories.items():\n",
    "        model['model'].save(path+\"search_\"+modelName)\n",
    "    histories = {**histories, **optimizersHistories}\n",
    "    optimizersHistories['Optimizer'+\"_\"+defaultHypers['optimizer'].__name__] = learningRatesHistories[bestLearningRate]\n",
    "    bestOptimizier = utils.getBestStudyFromHistories(optimizersHistories)\n",
    "    utils.displayStudiesProgress(optimizersHistories, path, 'Comparison Optimizer')\n",
    "    utils.saveStudyHistory({'BestOptimizier': optimizersStudy}, optimizersHistories, outFilePath)\n",
    "    print(\"Best Optimizier:\", optimizersHistories[bestOptimizier]['config'][\"hyperparameters\"][\"optimizer\"].__name__)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ----------------------------------------------------------------\n",
    "    # ---- Stage 7 - Train top K Best models on train+validation -----\n",
    "    # ----------------------------------------------------------------\n",
    "    \n",
    "    \n",
    "    \n",
    "    Path(topModelsPath).mkdir(parents=True, exist_ok=True)\n",
    "    topKmodelsNames = utils.getBestStudyFromHistories(histories, top)\n",
    "    topModels = {x: histories.get(x, None) for x in topKmodelsNames}\n",
    "    topModelsTrainedHistories = {}\n",
    "    for modelName, model in topModels.items():\n",
    "        history = model['model'].fit(trainDs.concatenate(valDs), testDs, epochsBest, verbose=1)\n",
    "        utils.displayConfusion(testDs, model['model'].getModel(), path)\n",
    "        model['model'].save(topModelsPath+modelName)\n",
    "        topModelsTrainedHistories[modelName] = {\n",
    "            \"history\":history,\n",
    "            \"model\": model['model'],\n",
    "            \"config\": model['config']\n",
    "        }\n",
    "    utils.saveTopModelsFullHistory({'FinalModels': topModelsTrainedHistories}, path=topModelsPath)\n",
    "    utils.displayStudiesProgress(topModelsTrainedHistories, topModelsPath)\n",
    "    return\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def researchStudy(study, path=None):\n",
    "    datasets = study['datasets']\n",
    "    studyParams = study['studyHyperParameters']\n",
    "    preparedDatasets = utils.prepareDatasetsForStudy(datasets, path)\n",
    "    bestModels = {}\n",
    "    for dsName, dsObj in preparedDatasets.items():\n",
    "        bestModels[dsName] = researchBestModel(**dsObj, **studyParams)\n",
    "    return bestModels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Datasets Configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetsConfig = {\n",
    "    'batchSize': 32, \n",
    "    'parallelTune': tf.data.AUTOTUNE, \n",
    "    'split': [0.7, 0.15, 0.15], \n",
    "    'inputDim': (224, 224, 3), \n",
    "    'seed': 9, \n",
    "    'shuffle': True\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2. Gender** \n",
    "##### **Datasets:**\n",
    "* **SOCOFing**\n",
    "* **NIST Special Database 4**\n",
    "\n",
    "##### **Hyperparameters**\n",
    "* Optimizers: *Adam, Nadam, RMSprop*\n",
    "* Loss Functions: *binary-crossentropy, mean-squared-error, hinge*\n",
    "* Learning Rates: *0.1, 0.01, 0.001, 0.0001, 0.00001*\n",
    "* Backbones: *MobileNetV2, ResNet50, EfficientNetB2, InceptionV3, Xception*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "genderStudy = {\n",
    "    'datasets' : [\n",
    "        ds.SOCOFingGender(**datasetsConfig, sampling=ds.SOCOFingGender.UNDER_SAMPLING), \n",
    "        ds.NISTSDB4Gender(**datasetsConfig), \n",
    "    ],\n",
    "    'studyHyperParameters' : {\n",
    "        'defaultBackbone': {'backbone' : bb.MobileNetV2, 'weights' : weightsMNV2},\n",
    "        'defaultClassifier': cf.DefaultClassifier,\n",
    "        'defaultHypers': {\n",
    "            \"optimizer\": Adam,\n",
    "            \"learningRate\": 0.001,\n",
    "            \"loss\": 'binary_crossentropy',\n",
    "            \"metrics\": 'accuracy'\n",
    "        },\n",
    "        'losses': ['mean_squared_error', 'hinge'],\n",
    "        'learningRates': [0.1, 0.01, 0.0001, 0.00001],\n",
    "        'optimizers': [Nadam, RMSprop],\n",
    "        'classifiers': [cf.AlexNetClassifier, cf.MobileNetClassifier, cf.ResNetClassifier],\n",
    "        'backbonesForSearch': [\n",
    "            {'backbone' : bb.ResNet50, 'weights' : weightsRN50},\n",
    "            {'backbone' : bb.EfficientNetB2, 'weights' : weightsENB2},\n",
    "            {'backbone' : bb.InceptionV3, 'weights' : weightsINCEPTIONV3},\n",
    "            {'backbone' : bb.Xception, 'weights' : weightsXCEPTION}, \n",
    "        ],\n",
    "        'epochsSearch': 10,\n",
    "        'epochsBest': 20,\n",
    "        'top': 5,\n",
    "        'verbose': 1\n",
    "    }\n",
    "}\n",
    "\n",
    "  \n",
    "# genderBestModels = researchStudy(genderStudy, dbStudyOutPath.format('GenderStudy/{}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2. Finger Name** \n",
    "##### **Datasets:**\n",
    "* **SOCOFing**\n",
    "* **NIST Special Database 4**\n",
    "* **NIST Special Database 300a Roll**\n",
    "* **NIST Special Database 300b - All Scanners**\n",
    "\n",
    "##### **Hyperparameters**\n",
    "* Optimizers: *Adam, Nadam, RMSprop*\n",
    "* Loss Functions: *categorical-crossentropy, mean-squared-error, categorical-hinge*\n",
    "* Learning Rates: *0.1, 0.01, 0.001, 0.0001, 0.00001*\n",
    "* Backbones: *MobileNetV2, ResNet50, EfficientNetB2, InceptionV3, Xception*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fingerNameStudy = {\n",
    "    'datasets' : [\n",
    "        # ds.SOCOFingFingers(**datasetsConfig), \n",
    "        # ds.NISTSDB4Fingers(**datasetsConfig), \n",
    "        # ds.NISTSDB300aFingers(**datasetsConfig), \n",
    "        ds.NISTSDB302bFingers(**datasetsConfig, subClass='K'),\n",
    "    ],\n",
    "    'studyHyperParameters' : {\n",
    "        'defaultBackbone': {'backbone' : bb.MobileNetV2, 'weights' : weightsMNV2},\n",
    "        'defaultClassifier': cf.DefaultClassifier,\n",
    "        'defaultHypers': {\n",
    "            \"optimizer\": Adam,\n",
    "            \"learningRate\": 0.001,\n",
    "            \"loss\": 'categorical_crossentropy',\n",
    "            \"metrics\": 'accuracy'\n",
    "        },\n",
    "        'losses': ['mean_squared_error', 'categorical_hinge'],\n",
    "        'learningRates': [0.1, 0.01, 0.0001, 0.00001],\n",
    "        'optimizers': [Nadam, RMSprop],\n",
    "        'classifiers': [cf.AlexNetClassifier, cf.MobileNetClassifier, cf.ResNetClassifier],\n",
    "        'backbonesForSearch': [\n",
    "            {'backbone' : bb.ResNet50, 'weights' : weightsRN50},\n",
    "            {'backbone' : bb.EfficientNetB2, 'weights' : weightsENB2},\n",
    "            {'backbone' : bb.InceptionV3, 'weights' : weightsINCEPTIONV3},\n",
    "            {'backbone' : bb.Xception, 'weights' : weightsXCEPTION}, \n",
    "        ],\n",
    "        'epochsSearch': 5,\n",
    "        'epochsBest': 10,\n",
    "        'top': 5,\n",
    "        'verbose': 1\n",
    "    }\n",
    "}\n",
    "\n",
    "  \n",
    "# fingersBestModels = researchStudy(fingerNameStudy, dbStudyOutPath.format('FingerName/{}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3. Fingerprint type** \n",
    "##### **Datasets:**\n",
    "* **NIST Special Database 4**\n",
    "\n",
    "##### **Hyperparameters**\n",
    "* Optimizers: *Adam, Nadam, RMSprop*\n",
    "* Loss Functions: *categorical-crossentropy, mean-squared-error, categorical-hinge*\n",
    "* Learning Rates: *0.1, 0.01, 0.001, 0.0001, 0.00001*\n",
    "* Backbones: *MobileNetV2, ResNet50, EfficientNetB2, InceptionV3, Xception*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fingerprintTypeNameStudy = {\n",
    "    'datasets' : [\n",
    "        ds.NISTSDB4Type(**datasetsConfig), \n",
    "    ],\n",
    "    'studyHyperParameters' : {\n",
    "        'defaultBackbone': {'backbone' : bb.MobileNetV2, 'weights' : weightsMNV2},\n",
    "        'defaultClassifier': cf.DefaultClassifier,\n",
    "        'defaultHypers': {\n",
    "            \"optimizer\": Adam,\n",
    "            \"learningRate\": 0.001,\n",
    "            \"loss\": 'categorical_crossentropy',\n",
    "            \"metrics\": 'accuracy'\n",
    "        },\n",
    "        'losses': ['mean_squared_error', 'categorical_hinge'],\n",
    "        'learningRates': [0.1, 0.01, 0.0001, 0.00001],\n",
    "        'optimizers': [Nadam, RMSprop],\n",
    "        'classifiers': [cf.AlexNetClassifier, cf.MobileNetClassifier, cf.ResNetClassifier],\n",
    "        'backbonesForSearch': [\n",
    "            {'backbone' : bb.ResNet50, 'weights' : weightsRN50},\n",
    "            {'backbone' : bb.EfficientNetB2, 'weights' : weightsENB2},\n",
    "            {'backbone' : bb.InceptionV3, 'weights' : weightsINCEPTIONV3},\n",
    "            {'backbone' : bb.Xception, 'weights' : weightsXCEPTION}, \n",
    "        ],\n",
    "        'epochsSearch': 10,\n",
    "        'epochsBest': 20,\n",
    "        'top': 5,\n",
    "        'verbose': 1\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "  \n",
    "# fingerprintTypeBestModels = researchStudy(fingerprintTypeNameStudy, dbStudyOutPath.format('FingerprintType/{}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4. Same Person** \n",
    "##### **Datasets:**\n",
    "* **NIST Special Database 4**\n",
    "\n",
    "##### **Hyperparameters**\n",
    "* Optimizers: *Adam, Nadam, RMSprop*\n",
    "* Loss Functions: *binary-crossentropy, mean-squared-error, hinge*\n",
    "* Learning Rates: *0.1, 0.01, 0.001, 0.0001, 0.00001*\n",
    "* Backbones: *MobileNetV2, ResNet50, EfficientNetB2, InceptionV3, Xception*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "samePersonStudy = {\n",
    "    'datasets' : [\n",
    "        # ds.SOCOFingSamePerson(**datasetsConfig, numOfSamplesPerSubject=5), \n",
    "        # ds.NISTSDB300aSamePerson(**datasetsConfig, numOfSamplesPerSubject=5), \n",
    "        ds.NISTSDB302bSamePerson(**datasetsConfig, numOfSamplesPerSubject=5, subClass='K'), \n",
    "    ],\n",
    "    'studyHyperParameters' : {\n",
    "        'defaultBackbone': {'backbone' : bb.MobileNetV2, 'weights' : weightsMNV2},\n",
    "        'defaultClassifier': cf.DefaultClassifier,\n",
    "        'defaultHypers': {\n",
    "            \"optimizer\": Adam,\n",
    "            \"learningRate\": 0.001,\n",
    "            \"loss\": 'binary_crossentropy',\n",
    "            \"metrics\": 'accuracy'\n",
    "        },\n",
    "        'losses': ['hinge'],\n",
    "        'learningRates': [0.1, 0.01, 0.0001, 0.00001],\n",
    "        'optimizers': [Nadam, RMSprop],\n",
    "        'classifiers': [cf.AlexNetClassifier, cf.MobileNetClassifier, cf.ResNetClassifier],\n",
    "        'backbonesForSearch': [\n",
    "            {'backbone' : bb.ResNet50, 'weights' : weightsRN50},\n",
    "            {'backbone' : bb.EfficientNetB2, 'weights' : weightsENB2},\n",
    "            {'backbone' : bb.InceptionV3, 'weights' : weightsINCEPTIONV3},\n",
    "            {'backbone' : bb.Xception, 'weights' : weightsXCEPTION}, \n",
    "        ],\n",
    "        'epochsSearch': 5,\n",
    "        'epochsBest': 10,\n",
    "        'top': 5,\n",
    "        'verbose': 1\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "  \n",
    "samePersonBestModels = researchStudy(samePersonStudy, dbStudyOutPath.format('SamePerson/{}'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfmetal",
   "language": "python",
   "name": "tfmetal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
